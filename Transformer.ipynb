{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOr/pyk0hpskOHVUyiBYk/n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZhengyuanCui/Deep-Learning/blob/master/Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DqweBw9uYvJZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self, embed_size: int, heads: int) -> None:\n",
        "    super().__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.num_heads = heads\n",
        "    self.head_dim = embed_size // heads\n",
        "\n",
        "    assert embed_size == heads * self.head_dim\n",
        "\n",
        "    self.key = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.query = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.value = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "    self.fc_out = nn.Linear(self.embed_size, self.embed_size)\n",
        "  def forward(self, key, query, value, mask):\n",
        "    N, S_k, T_k = key.shape\n",
        "    N, S_q, T_q = query.shape # minibatch size, sequence length, embedding size\n",
        "    N, S_v, T_v = value.shape # For decoder, S_k == S_v but not S_q\n",
        "    \n",
        "    k = self.key(key.reshape(N, S_k, self.num_heads, self.head_dim))\n",
        "    q = self.query(query.reshape(N, S_q, self.num_heads, self.head_dim))\n",
        "    v = self.value(value.reshape(N, S_v, self.num_heads, self.head_dim))\n",
        "\n",
        "    energy = torch.einsum(\"ijkl,imkl -> ikmj\", [k, q]) # (N, S_k, heads, head_dim) * (N, S_q, heads, head_dim) -> (N, heads, S_q, S_k)\n",
        "    # Above computation means that the head_dimension and heads should be the same for src and targect value\n",
        "\n",
        "    if mask is not None:\n",
        "      energy = energy.masked_fill(mask == 0, float(\"-1e20\")) # to make this work\n",
        "    attention = torch.softmax(energy / self.head_dim**(1/2), dim = 3)\n",
        "    # (N, heads, S_q, S_k) * (N, S_v, heads, head_dim) -> (N, S_q, heads, head_dim)\n",
        "    out = torch.einsum(\"ikjm,imkl->ijkl\", [attention, v]).reshape(N, S_q, self.embed_size)\n",
        "\n",
        "    return self.fc_out(out)"
      ],
      "metadata": {
        "id": "eB_AVO-GY_eK"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super().__init__()\n",
        "    self.attention = Attention(embed_size, heads)\n",
        "    self.norm1 = nn.LayerNorm(embed_size)\n",
        "    self.norm2 = nn.LayerNorm(embed_size)\n",
        "    self.linear = nn.Sequential(\n",
        "        nn.Linear(embed_size, forward_expansion*embed_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(forward_expansion*embed_size, embed_size)\n",
        "    )\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, key, query, value, mask):\n",
        "    out = self.attention(key, query, value, mask)\n",
        "    x = self.dropout(self.norm1(query + out))\n",
        "    out = self.linear(x)\n",
        "    out = self.dropout(self.norm2(out + x))\n",
        "    return out"
      ],
      "metadata": {
        "id": "RuZVEzfPNEA1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      dropout,\n",
        "      forward_expansion,\n",
        "      device,\n",
        "      max_length\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.embed_size = embed_size\n",
        "    self.device = device\n",
        "    self.content_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
        "    self.layers = nn.ModuleList([TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)])\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, mask):\n",
        "    N, seq_length = input.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "    out = self.dropout(self.content_embedding(input) + self.positional_embedding(positions))\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, out, out, mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "hRJ47ByQHNAO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "    super().__init__()\n",
        "    self.attention = Attention(embed_size, heads)\n",
        "    self.norm = nn.LayerNorm(embed_size)\n",
        "    self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, keys_en, values_en, src_mask, trg_mask): # src_mask: pad to achieve equal length\n",
        "    attention = self.attention(input, input, input, trg_mask)\n",
        "    query = self.dropout(self.norm(attention + input))\n",
        "    out = self.transformer_block(keys_en, query, values_en, src_mask)\n",
        "    return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      trg_vocab_size,\n",
        "      embed_size,\n",
        "      num_layers,\n",
        "      heads,\n",
        "      dropout,\n",
        "      forward_expansion,\n",
        "      device,\n",
        "      max_length\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "    self.context_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "    self.positional_embedding = nn.Embedding(max_length, embed_size)\n",
        "    self.layers = nn.ModuleList(\n",
        "        [DecoderBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)]\n",
        "    )\n",
        "\n",
        "    self.linear = nn.Linear(embed_size, trg_vocab_size)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, input, encoder_out, src_mask, trg_mask):\n",
        "    N, seq_length = input.shape\n",
        "    positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "    out = self.dropout(self.context_embedding(input) + self.positional_embedding(positions))\n",
        "\n",
        "    for layer in self.layers:\n",
        "      out = layer(out, encoder_out, encoder_out, src_mask, trg_mask)\n",
        "    out = self.linear(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "O16_u5SPRAFC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      src_vocab_size,\n",
        "      trg_vocab_size,\n",
        "      src_pad_idx,\n",
        "      embed_size = 256,\n",
        "      num_encoder_layers = 6,\n",
        "      num_decoder_layers = 6,\n",
        "      forward_expansion = 4,\n",
        "      heads = 8,\n",
        "      dropout = 0,\n",
        "      device = \"cuda\",\n",
        "      max_length = 100\n",
        "  ):\n",
        "    super().__init__()\n",
        "    self.encoder = Encoder(\n",
        "        src_vocab_size,\n",
        "        embed_size,\n",
        "        num_encoder_layers,\n",
        "        heads,\n",
        "        dropout,\n",
        "        forward_expansion,\n",
        "        device,\n",
        "        max_length\n",
        "    )\n",
        "    self.decoder = Decoder(\n",
        "        trg_vocab_size,\n",
        "        embed_size,\n",
        "        num_decoder_layers,\n",
        "        heads,\n",
        "        dropout,\n",
        "        forward_expansion,\n",
        "        device,\n",
        "        max_length\n",
        "    )\n",
        "\n",
        "    self.src_pad_idx = src_pad_idx\n",
        "    self.device = device\n",
        "  \n",
        "  def make_src_mask(self, src):\n",
        "    src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2) # (N, 1, 1, src_len) need to think more, all\n",
        "    return src_mask.to(device)\n",
        "\n",
        "  def make_trg_mask(self, target):\n",
        "    N, trg_len = target.shape\n",
        "    trg_mask = torch.tril(torch.ones(trg_len, trg_len)).expand(N, 1, trg_len, trg_len) # target mask the lower half triangle of the matrix size trg_len by trg_len\n",
        "    return trg_mask.to(device)\n",
        "  \n",
        "  def forward(self, src, trg):\n",
        "    src_mask = self.make_src_mask(src)\n",
        "    trg_mask = self.make_trg_mask(trg)\n",
        "    out = self.encoder(src, src_mask)\n",
        "    out = self.decoder(trg, out, src_mask, trg_mask)\n",
        "    return out"
      ],
      "metadata": {
        "id": "1YAToZIZNJAq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: to check it runs\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# A random input example\n",
        "src = torch.tensor([[1, 5, 3, 2, 4, 8, 6, 8, 7, 9], [2, 3, 4, 6, 2, 9, 8, 7, 0, 0]]).to(device)\n",
        "trg = torch.tensor([[4, 11, 8], [3, 5, 7]]).to(device)\n",
        "\n",
        "src_pad_idx = 0\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 15\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx).to(device)\n",
        "out = model(src, trg)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "id": "h4rtRH7J7mL_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "686b8cb7-65a8-474c-fdfc-1faf18cda5b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 5.0398e-01, -1.0023e+00,  5.6857e-02, -2.6799e-01, -6.8624e-01,\n",
            "          -8.7852e-01,  1.0655e-01, -2.1558e-01,  8.1713e-02,  4.1557e-01,\n",
            "           6.1862e-02,  1.1059e+00,  7.6783e-01, -4.6962e-02, -4.9102e-01],\n",
            "         [ 7.2262e-01, -8.1058e-01, -3.1346e-01,  1.9133e-01, -2.6969e-01,\n",
            "           7.3957e-01,  4.8730e-01, -5.3206e-01, -1.3396e-01,  3.1679e-01,\n",
            "          -7.5704e-01,  6.7768e-01, -3.3729e-01,  7.5567e-01,  6.1345e-01],\n",
            "         [-1.0473e-01,  1.8362e-03, -6.5824e-01, -5.5786e-02, -1.0601e+00,\n",
            "          -1.2123e+00,  7.6936e-01,  1.5073e-01, -6.2403e-03,  1.1964e-02,\n",
            "          -3.0024e-02,  1.9799e+00,  1.8055e-01,  2.4357e-01,  9.7782e-01]],\n",
            "\n",
            "        [[ 7.8722e-01, -1.2320e+00,  4.7462e-01,  4.7971e-01, -1.0315e+00,\n",
            "           2.4525e-02,  1.7244e-01,  7.5950e-02, -7.6394e-02,  7.2079e-01,\n",
            "           4.5045e-01,  4.7763e-01,  1.1796e-01, -8.4385e-01, -4.9978e-01],\n",
            "         [ 3.9467e-01, -4.5988e-01, -2.0479e-02,  2.6329e-01, -7.0352e-01,\n",
            "          -2.5410e-01,  2.4623e-01, -1.2866e-01,  4.0591e-01, -1.3057e-01,\n",
            "           3.5355e-01,  2.8014e-02,  1.5841e-01,  1.0883e-01,  4.9613e-01],\n",
            "         [ 2.3143e-01, -6.0063e-01,  1.9088e-02, -1.6466e-01, -8.2650e-01,\n",
            "          -8.7580e-01,  3.0070e-01, -2.5868e-02,  7.5952e-01,  2.9400e-02,\n",
            "           5.3069e-01,  9.4777e-01,  2.6069e-01,  1.3890e-01,  3.2788e-01]]],\n",
            "       device='cuda:0', grad_fn=<ViewBackward0>)\n",
            "torch.Size([2, 3, 15])\n"
          ]
        }
      ]
    }
  ]
}